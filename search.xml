<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[工作の思考]]></title>
    <url>%2F2017%2F07%2F16%2F%E5%B7%A5%E4%BD%9C%E3%81%AE%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[No pain, no gain. U2FsdGVkX1+aRy0nnSOrJ5nXWeSZYxtVN0/e8tEszNWwztnzsQY+q0TuU0bBqkEObvMS3OaT12bjdM/Emm62aG4IplFEZf1g7wDZsVFUTOcq+mi2D2UqbWkKbW+PjlYyYMZwnULPIuSrSzw0IY+K1Zo6llQcRjaafu7SJc+ewYW+naebaPJk4FyhWD1T5PI/+OTysNzT4FV+6sT0KkdCJZqQsMhkQEmdiSCdfeCqlSquJ35WqMVqORCsZx5ABFkGr+eGrc15H9Bj3lt/Kks8tFwk8E4rRLl3R32Q9nRHb0RbdPRYem98njK2us7x3l9PPx4uxdgfx8DFOX0nX5Xna5PBMGZ1ZjoaBczNc+4N6vmjflXKfH+Clk965in9iu/JxOzL0qH6jSeMzZDPbufuNRiPDhgK/D9NHkzPwzfytNye40fNhncVMoJuLVGwKph5nLvjdCGdhd3ExmAYZiZ8JfkjDwgWrwMuZFQDaAchtU/7qabkQGCJa6+Qn4jMGUO42zfhXsxrkO4jXz1RL2fStBNGHP02zc1R1tTiqdtLWuyKdB2s4AfmisJXG7wT8R+73H4Zb57+xTdtpDl2+K8exbnqcS4xKdgkJl1OcFyuvhtmVfdiJh422zKRj7Vf27buUw5J1gx/WlwPDkM15vVEfhMyb/vPhgl9wOt+zvQcbbRBK+Iw5DiuyMrCYhun07oemfVyCwzXbm6SdJW/oGkUrUtKeaOuUm+Kn22mymTKOUSBcPa/h5YdMt7r5WNYFcOBnNeJc5EYG8XcAlzMANUywIbfl3eMduR5Eo7FpF6TdIEOKYpy+aBjuHRZUUF//+vSalc4rFW+/+QelMzkZjaCwBWmE2MPNRMhpjaYuTWUOZbQW3ZHHly6zMtzE7G8meiYcDXw4PJESbbasEP+9AMPEiD14lcAhCo6sb3xhGayO5UzwQpVrMCsDoeHmmKrtKpYoBsDr+NkAwoRMXpdK/NMTdCxsES5smHiknUfLTZ+huKvSAmT7n/71k6luT3CT4yib6pXgObJOAL9FUA66Yr/OSYHpz/v0j08Jb71WpuMUJ3ErXoQZBG4rTdSkIk9ukuJXb5JqaIdH4oVMHU111Ww+z0y48vg5dCHKw2rYtfYXdlHv7/d7vu96cqu/f0ggBIIzSdW+1MFVBv3F+p8JrEPiErNEyZEOe6K2+CT8ec34vfWNM5kLIQyoVQVN5aJC2ejPofFIpJKWfhupfF+xw1FAolRS+DGwhjDO2oNjvttRj1SrgkFzrBtIZ0uT3wsevlSD60dAPU503S+NWGu2BFyQCg5BPWP1UOyPIGpEGoc8n64LzeAE5kdzIYFb+H0Lhf5tCYFDGrR9OV3scISmyYkRm1P3DhIb8mjkNQqnqW3CM9z6VXcV0Tbxaxa9a6xTlO11N3wEK9quTMMXHLPBk/uJN1nKY6fNfnYo/BG0MFdwjns5/OvLUM13Fmn/XcWHLg6QCjp+14v8D5qlqnXTUZvtYUMK+nvBGO/4KAGU4JAnC3maK62U1AnDMJsZZz/mn7KDDW56a8TaJEtbT65zscPRSqhRJklyxzlJ09FSodsVyCCr1nsn7PW4fpzVAX94BLVOA/6JgqIH8dYplX+zZ5axU1V9qGGxh7Fv1T5g6k4UTDwyrrqdqVbMQ2TjvBvhQ8Vxzkn5hSIRwC6o1p7Ny1zXfFD3SuDHhm+OCbZUrp2Vc6u81QgLd9DGM7Mh32guZkqAMN+bPtAmHlnuf3ti39vYq2vo6lYZ7VjRQIBx/fIZDVg02Fhe+j/TXy9N2n6n46QnjKkLb/QmIkID3DAhYnmsJblR8l/IxDAU3jxqEwwN/6eZLgjfjBZeHy8Rv7jcpUuwSmT10sPWZ1YxAZLB682mYf7BisJ/i9cmpY+uRbMkXiUfR6kI90zsY84fNiIm3TGPCr+dvXddB+wWdDDrydo1fUuHnM6lN0kLyNmhOojz74sXTcHKByFCQLWFW0vsgS/mARTNfj+9jSi1dDQxvyc6J2652XWJVXq/oVSGGdKguUF0mOaFWVm3Azms9lsWSIvrf0eV04IHq//YnoDXst5e9W9lWi5t3ZAIkyWtm9lcf9jWQ5NQuciu55jbTgFk7CATemeiLxK9XmwGUH8mn+uQVQ+Sb1BhsdgDGKJ9H3pGdDD7hNhSaKLZ8KEVhwFVi/ZWuXzFmM5dtX32wKF2WHth+laUBzaX42sGdxV1klS42NFE9EFp9cmeSeUjXtLS9vh5SRAc5o+yEktbIO00kUc6UDTCm2QfSX2m3dIjwVc24HEr2HOrPmbJMErV2Hr3Sq17D+SD6tO+aFtDe2hLVW7AZblZAPJmu9zGE4FbCXw0OEEcj7TWZWQtudXs04gn2fVzavrN+c9zAiyZ6bGLPGsl26scqRouGVv4dBXXM7/113WooimHVQnkxp9ht1sPxFbc04/xBptxMcQc96mwXZ0mrYYXfETeFxNfXR4YviGcC5xKxln4Z2YAeVpjTQST714gtsT/olAkGBpKG8exTJy0W3ZUOm3jfirFRoCfEscWq6rFqbgesl8rKwinPRtYmO1NgyQs667bGwcZAUp68paPjhVnPasiHLESAbCfM2VkYsJvexiOy6osg7clE+JGX+LqSAr/6PX6C7/xfiGaZBTY3WcgPw2hEh38EEEJCI43yvHBcpCkNqLDUPRAvzvkTzebz2rSv0VtA/Kqzr/k8+k50REVZTEZyGw9K5az1+m7NltI4j75pcGFtzDNpndFaN+Lw6F82YPGFHBcOTT4pNYm2/TIa0G7/IpnXuS/Tcr9HcFdAEh7ACvxK7qvd7vPHm9lOb/ZcX+IKF+22lUuBeICGQkftTxXPnG0+NKaVjHT9Lrh6tAdWD3x1mSu9sPe/cfYyM3i14oaM7RYW9q8SBQd4zZycuFQ29ujfRbdN7k+EnS2mygbjwoUogJHOmuVrngn6VUUBymAZ+fQfr5oc9CIO2qk5jlw3IXfzhR02JCUMMssUwHXNY1YC71/0C7PSVONV89rpuakL2JOKeJZK6+yxSPSqeaaHOZHmUPmtg9J8qHcsu8Fjpn4QtLRACK1d+003mLo0hesiVi2ycdj/qwPQOdKJFqWk2clIdVDw1VDGlAbwQD5N7cLwBvBm6uHKJHhhlZyBOawY7AV+wHZwfaVg5VTYW1PQ6pt3vqL/0Ejdogtl2lcLejq96LjpL5iA7CJnQeWaJpOMrNeN9snmbmZeRG1CY1XoUTnLVsZrJ+68Hp0ZnAUf8NJ+Xdj7dz/WhNpTWfHHieWwbV03S4zg/HDUGpOdvLnFfwdxzjTjH0XTT1Y7SNfkYQEv1WGzHQf2EEAIrnNshTqCv4y21cZ2UmeHX5DJbmevxmYKVyqOs1r5M1fUwQBnK/yMmCVXqsbFOrFI9EPk++woueQKEfTPDGXwYenzP/U31mKNL0btljGNk9ri/WvLmoApkRlZnJpdr+D2ti1LRdk2L7xdVWIVeHifXw6KJw0HNBpnJHeSnUNzLpOI/zMMNsiKlk1gXPpUfthVflQrgRzPgjHnno7hoYZ53l3d7gKFvnmZXJbPd+8O2Mld7UDaKmt6A5EYrSxssfNLZmG8Hz6LG2KLT9JxT0uM/rBN1sbbdJFz8HGs+hnQfpWEWzwRkC3xNlIvWe48RRZhpemAEy+FSMpm67QNpxFw8enpsj6ehiwlCriVl/jtkhvspqNqnSxVpN6OwCY/7iovmZXr6SBg2XLKHp96yCc3A6wjTS+gz2JFlNeFM6i2hyjnpNFXk8AxD8x1+9Kl3ds6n0r0wfANhZw0qnaLiw/W5zYtw3o5KMcZHHrUDsArHREEVQSqWN3LnFwTtgwRhZ9yAFW9gEtTctR6ioGA13h79exSGFOcrG0V19PLJ4vAZICnPpIKXDnoEV2e9cf7AMLYqltPk2Gm/DKDSfYA2m+ieSo9vOaQLa+Yb9ER2ED+2IbzQlGTsrfI3dQrTegA4GMBNqwsdJH2YWRXMmwFiPqqQwTbP4TmsLXdNDAzAKvurw7/AWz9M0EAU9zVIpZl698xXhedmHYJ79NQz6ab2I22wEK4I8+jMCpN6MOm0huFzKsFhoxaRgN76apxCvOLfvGJ/HO7XXWhaoEI5BMrygGy0VlAhLc2I1UgrekenNhnf00WasB1ds91liDpQI9oFKYmUlqyq7Q/t6CFpxKlc+vK45JSpSAupMwMvRHEvbg0gwv5WgN+J5nLMk8rwkPzWWMoPIyxbsPoxkEIQWs7ugG7JKu15C5LSPArw4KvDgOs8kEBwGaeh5MTfNz8xuML4tS/R4RiMzdlCIzPxGyMQ7vJVpKGa6P71zPsjZwtU/4i5f9a/P4q8H7SO/4Brfd8vHN38uSSYBhEUQjH1qvbrbf5k/06qBxGp9iKGo+wFHOXHArXJqGmPH8EcVRcP6dheQn5KrC1sVGUGM1G8JiFN+BoKQa4iLNAuCKUWDBUospXrvxCq6NQ3RGNkLw6Ngl+oHRW/lz/ezzD9pgqoPGDenP/i6HKGj1Cqf3LP2gHJlHwfhw1Ee1wKRke66pDCvXWVzb+Faal8fyEXTL1j2pIbIucHboLgbhOaC1lfoRsJBSScUoueCRWQ9G7x8Ii601mrvbGtyqRZqd86BiaLDR3137fLsr3VsejitYljIraUjBORp8L8+SRCDtEfwmC1d2jiSeZ8Wj/8hKAaBPhBAsuGIxzOKUdHRl1lIWbZ807ijH5IwZGCzBYwu3YPHGOe1r1BSe0mW1taNy65yJbqSwWujFHN9ZuLHRkB1BnOXecW6TnSpty32qLc8FHI/gsj6y/WuyVwvuSKsoTovRNoVExPu2SGjd/YggvGNU1v+4K1Pe7lwNMUiAZDnu8XP0eYNGHu1OBFaZsKDCccw4m1sFSNo/NtPIqUlW8IKBQtoEJUCyh9q60ZXh9kTe1vIcaSW/Pfd5nBOKEkt3JcUWmcng3BUu/2K85o1QI68Xz0DddkLua+VVKGNmqhEWd+B1nU/602yIE+WRwr1JxLRh88SWJft1htjdLsuxAwkM+zuw8pGsuO4nVPRlVZTh9sN1EpH5DPMQy5m2uTj/OsMH7bNvFE7E+5E1a1DjOMG9JNZWT2K+YuX7jgeP12b7UFWT6hzT9QJTZ5dOBP5Kea9JnxoUB8IE7oS3vnJ0v1DBcGGQ710ptgK/46kLd3GTV64M1SKyxR3nZaUZM58Xn95fBBc21Un/cbQx1lJ9AXBv0erPaUm332Tke/hORjtQlLO/vYZMvHeK5Q1xXlsH8/myTZUsH8TJ8XE8KSxsIQbh8YZXEo1x0ny+hxBn+Mcfm+cf2hA3XrxZSpnlTv9cI0L8FyKBSEXhS1dH0KfZzBRM7W/5mZ93q7VynmYX53/JEzD2jC9Ym0gXTpzrSOYoE+TyOprs9aEUcQRtm1idD061ZJcLMMkruMK9k5TlcKarJmDan6ePEV9iduoumsE33HkhNOAjpXlAOT2+fRa6wNg9o1DKkXgbqkubSXzdkyOCzbeB1siRq+jL8Y9a5tKUUADp47eTM0PPZO6Kyt2I7kS80i/NSe7b0OT2hxcRiQD+1YiDle/jpq47mK8gRV/rNIYfxgztXUWejg2q9oc3kW6INHBO6d96hjT9GTrPzFguewOHyqDFcLc0t0wd1mI4vZSgMeHdo+3/Df0HAOU24dTBH/wcWswyA6NT5FT7M84TZjGSpveY1BcMiTs74XlbgxpIGV0WKoprDy8/FYD+5AlWfBQM2ksL/ANGHOaPAAm35OlocCfsC/ICE2a1q51/PzuWAdE/2KYA/ew32tbBd4k7ltwEZPzf6uULBfgo1lsQNd5HDnJFGw4+lqy20mKqZqJRGungB56zWjOjUWUELWksDa3iiJz0hl1/GHg6kuKfkEb3flyH1vI2wr6ua/ZXwloPnuEWma46kjAi1ZWVvBN5xiBAbF6o18a85DCexHtiD3MhaI3LcHw+sy1bBF3uUg1nMefCOg0VWTNhFFCZ0WfbXvDDIzgdyQEqexgUwFNssBz8mONePs01HVyU1/0hAtkEZgcW/RNFZw5PnBYtHBmHhz6eDfsNxvjhdIZJ65UIj9aanJFM4HAdLLQW+oYFLCYu7UBhM+H/LfgbcrfG8Et2c0b/g0FQyHWik7B2kyDrcqSpoJYtfYokM/p+7GM1YbRZXVE9z3/LUKG5Ai7Yd7fJyI/RnN7MSFTobdR79TeohZqezr9kyyklHraIbuTe5gmiO5RU0fWWA/DxRNGNZdARcZ6/kdzGoD753duq+2yBFX4lXkjwX8PwZikT3+LergMBTmTJQscnAlH5OXB1cO1YyxsrOUK/AyP6RSGj+9tFMKtWwB3FeCW8LtiSVVJnjN4qg3m+ee+rzqgps2SbZJSim9erpp376J2r1JviwGPQEgUliKGwsmoIh2FSt7EtovdzD9m2CL+YbLejP7Ck0nHW8Is6hZM6clgT/qzWkgQlFBtjo0E8opXookDrISloLYc0VxEr+7hkVVL9QMYjO/PXtDFCmXFGYJn+iCr3w7uBLMX+Fqpd4E4FLqaW5AXmgzb/w2an9U80pHyl1qyxZ6X4zL+u+/MVEgWwnlwoTG82c6YGTDlOY/6cch6A97NjTZL1MC0IeuY2ALtKO2hu/YF762u1Tmb/Y71j3yj15dx99x+m3Gqfc8/5fU+YF+0xJK8kmAgTinFEytNrJDi4b8gmr95FM+aOLhDCUZaJGlpGG04XPjyrcIHD/Wu7lIbHWCf0BB+7Ws3YAZlPIqmegEbhwDnqWB0wFMG1eh3jPWm2c8ewjK2ZLYvAzeWwwfSnvYMyNqBLgINmpofRrF6DOdn3F8/hS8zvRu0Nd3SilT6OOTc9dN07Pb67TK+ZV3m7rgrRYlTlvSpLCbQCeMsc2MATG0Q1YsT6IA4z3nn+kVeHmNE97Q8COwG]]></content>
      <categories>
        <category>职场</category>
        <category>合作</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>为人处事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Scan Too Slow in Django Application]]></title>
    <url>%2F2017%2F07%2F16%2FDjango-Redis-Scan-Too-Long%2F</url>
    <content type="text"><![CDATA[Recently I met an issue that one of our APIs of the online production servers is very slow due to redis scan. From next graph generated by NewRelic, we found AVERAGE response time is 2930ms while redis scan could take 2680ms, which is 2680/2930 = 91.4% portion of the total time. Why redis scan cost this much? From our local environment, we never noticed such huge performance issue. 1. Locate the culprit1.1. PhenomenonAfter that I dug into the source code and found next code snippet in our Django Application and this is the only code of this API to access redis, so this must be the culprit of this issue. 12345from django.core.cache import cache// ...... more code is ommitted herecache.delete_pattern(CACHE_KEY) From the above code, we could know that redis is treated as a cache server here. So what does delete_pattern do on the earth? So I traced the code and in the end found next delete_pattern definition from django_redis.client.default.DefaultClient#delete_pattern. 1234567891011121314151617def delete_pattern(self, pattern, version=None, prefix=None, client=None): """ Remove all keys matching pattern. """ if client is None: client = self.get_client(write=True) pattern = self.make_key(pattern, version=version, prefix=prefix) try: count = 0 for key in client.scan_iter(pattern): // please note here client.delete(key) count += 1 return count except _main_exceptions as e: raise ConnectionInterrupted(connection=client, parent=e) The code calls client.scan_iter(pattern) to get next cursor and try to delete that key of the cursor. 1.2. Redis scanFrom the official site of redis scan, we can combine MATCH and COUNT in the SCAN command.123456789101112131415161718192021222324252627282930313233redis 127.0.0.1:6379&gt; scan 0 MATCH *11*1) "288"2) 1) "key:911"redis 127.0.0.1:6379&gt; scan 288 MATCH *11*1) "224"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 224 MATCH *11*1) "80"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 80 MATCH *11*1) "176"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 176 MATCH *11* COUNT 10001) "0"2) 1) "key:611" 2) "key:711" 3) "key:118" 4) "key:117" 5) "key:311" 6) "key:112" 7) "key:111" 8) "key:110" 9) "key:113" 10) "key:211" 11) "key:411" 12) "key:115" 13) "key:116" 14) "key:114" 15) "key:119" 16) "key:811" 17) "key:511" 18) "key:11"redis 127.0.0.1:6379&gt; The above command returned me with a 0 means, in the first line of response, there is no more data to scan. If not 0, the number is next cursor to be used to scan. It is important to note that the MATCH filter is applied after elements are retrieved from the collection, just before returning data to the client. This means, firstly it will retrieve the data, and then will use a filter to match the retrieved data.Does this means there could be multiple back-and-forth between the client and server, and which will cost the huge latency of this redis scan? If that’s true, we just need an extra easy COUNT parameter. 2. Validation in local environment2.1. KEYS numberHow many keys are there in our online production redis server?123127.0.0.1:6379[10]&gt; INFO# more code is ommitted heredb10:keys=6986,expires=6986,avg_ttl=40118300 From above code, we can know there are nearly 7000 keys in our server. 2.2. Client-Server latencyI simply use the ping command to get the latency and it’s 3.5 ms. Due to classified cause, sensitive info is hidden.1234567[xxx@xxxx.xxx.xxx.xxx ~]$ ping xxxx.comPING xxxx.com (xx.xx.xx.xx) 56(84) bytes of data.64 bytes from xxxx.com (xx.xx.xx.xx): icmp_seq=1 ttl=58 time=3.58 ms64 bytes from xxxx.com (xx.xx.xx.xx): icmp_seq=2 ttl=58 time=3.46 ms--- xxxx.com ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 3004msrtt min/avg/max/mdev = 3.461/3.498/3.585/0.050 ms 2.3. Simulate the Network Latency LocallyFrom this post, I successfully simulated the network latency.1234567(venv) vagrant@localhost:~ $ sudo tc qdisc add dev lo root netem delay 2ms (venv) vagrant@localhost:~ $ ping 127.0.0.1PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=4.34 ms--- 127.0.0.1 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 4.348/4.348/4.348/0.000 ms So I made the lo NIC as slow as around 4.3 ms. 2.4. Measure the time of cleaning the cache before and after setting the delaySince this is a Django application, so I can use Django Shell to try to execute the clean cache statement, which as a result operate the local redis server.12345(venv) vagrant@localhost:~ $ time python manage.py shell -c 'from django.core.cache import cache; cache.delete_pattern("view_cache:dispatch:123456789:category_slug:all")'real 0m8.404suser 0m0.944ssys 0m0.696s What if we don’t set the delay option, what’s would the timing metric be?12345(venv) vagrant@lcoalhost:~ $ time python manage.py shell -c 'from django.core.cache import cache; cache.delete("view_cache:dispatch:123456789:category_slug:all")'real 0m4.366suser 0m0.800ssys 0m0.648s From the above test, we can see that before and after will make a huge difference as large as 4 seconds !!! And this is under the network latency of 4.3 ms. If the network latency is around 3.5 ms, the total time of cleaning cache could be as large as 3 seconds, which is very close the above phenomenon! Now, we can get a conclusion: network latency will influence too much on the redis scan performance. SCAN will call the redis multiple times back-and-forth, which in the end, cause a large redis scan time. How to solve the problem? Use the COUNT parameter! 3. Solve the problem3.1. Estimation of deleting time againFrom the definition of delete_pattern method, we can see it just scan_iter the redis using the default COUNT which is 10. Let’s give an estimation here: The Redis Scan use a sequential scan with COUNT as 10, there will be 6986/10 = 700 times to scan. Each scan costs at least 1 ping time, which is 3.5 ms. In total, 3.5*700 = 2450 ms, which is close to the 2680 ms of the phenomenon above. Interesting. From above estimation, we are more confident of our guess now, and using the COUNT must be the solution! But the delete_pattern method provided by the default client doesn’t use COUNT parameter, so what I can do is to create a new Redis Client which is inherited from the default Client and override this delete_pattern method to use the COUNT parameter. Here is my code:123456789101112131415161718192021222324252627282930313233import socket# Compatibility with redis-py 2.10.x+from redis.exceptions import ConnectionErrorfrom redis.exceptions import ResponseErrorfrom django_redis.exceptions import ConnectionInterrupted, CompressorErrorfrom django_redis.client import DefaultClienttry: from redis.exceptions import TimeoutError, ResponseError _main_exceptions = (TimeoutError, ResponseError, ConnectionError, socket.timeout)except ImportError: _main_exceptions = (ConnectionError, socket.timeout)class CacheClient(DefaultClient): def delete_pattern(self, pattern, itersize=None, version=None, prefix=None, client=None): """ Remove all keys matching pattern. """ if client is None: client = self.get_client(write=True) pattern = self.make_key(pattern, version=version, prefix=prefix) try: count = 0 for key in client.scan_iter(pattern, count=itersize): // have a itersize here client.delete(key) count += 1 return count except _main_exceptions as e: raise ConnectionInterrupted(connection=client, parent=e) When I call the code, I make sure to pass the itersize very large, say, 10,000 to eliminate too many back-and-forth RPC calls. 3.2. Enable the customized redis client classOf course we need to enable this client in our settings/local.py123456789101112CACHES = &#123; 'default': &#123; 'BACKEND': 'django_redis.cache.RedisCache', # redis server 'LOCATION': os.environ.get('NOJ_REDIS_CACHE_URL', 'redis://localhost:6379/10'), 'OPTIONS': &#123; 'PARSER_CLASS': 'redis.connection.HiredisParser', 'CLIENT_CLASS': 'common.cache_client.CacheClient' // enable the customized redis client class &#125; &#125;&#125; 3.3. Test after fixingI added a 10000 after my delete_pattern call.12345(venv) vagrant@localhost:~ $ time python manage.py shell -c 'from django.core.cache import cache; cache.delete_pattern("view_cache:dispatch:123456789:category_slug:all",10000)' real 0m4.021suser 0m0.784ssys 0m0.612s The time now is very close to the time before setting the network latency, which means, delete_pattern won’t be a performance issue anymore, since it only has one RPC call now. Cheers, bro! 4. Conclusion Don’t be afraid of the library source code, just dig into it. Use tc command to simulate the network latency. MATCH inRedis Scan is to filter on top of the retrieved data. Default COUNT of Redis Scan is 10. Use python manage.py shell to execute ad-hoc test. Use ping to get network latency. Use INFO to get redis statistical information. References Redis Scan tc: Adding simulated network latency to your Linux server]]></content>
      <categories>
        <category>Database</category>
        <category>NoSQL</category>
        <category>In-Memory</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Redis</tag>
        <tag>Scan</tag>
        <tag>Performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debug Ansible and Tiny Introduction of PlayBook]]></title>
    <url>%2F2017%2F06%2F29%2FDebug-Ansible%2F</url>
    <content type="text"><![CDATA[Ansible is a popular dev-ops tools for us to execute ad-hoc commands immediately on large mounts of machines in parallel which accelerate our working speed. It’s simple but powerful and compatible with different OS platforms. Even more, it has lots of pre-defined modules for us to use, which significantly make the dream of reusing Dev-Ops scripts come true. However, when you’re goging to use this fantastic tool, how to debug when you’re executing the ansible play-book with flow of commands? It looks like a unstoppable flow. So we need two things for debugging ansible playbook: Stoppable Print Debug Message This article will also give some tiny intro about play-book. For more info, please review the official documents. 1. StoppableSo, here we have a trick, using the fail module to stop the execution process. Add next code snippet at wherever you want to stop. 12345# more is here- name: "STOP ME" fail: msg="This is the debugging stop" when: 1==1 After that, you could see the message like below:1234567891011121314# more is here ... TASK [pre-ansible : STOP ME] ******************************************************fatal: [node-1-master]: FAILED! =&gt; &#123;"changed": false, "failed": true, "msg": "This is the debugging stop"&#125;fatal: [node-2-slave-1]: FAILED! =&gt; &#123;"changed": false, "failed": true, "msg": "This is the debugging stop"&#125;fatal: [lcoj-judger]: FAILED! =&gt; &#123;"changed": false, "failed": true, "msg": "This is the debugging stop"&#125;NO MORE HOSTS LEFT ************************************************************* to retry, use: --limit @cluster.retryPLAY RECAP *********************************************************************lcoj-judger : ok=5 changed=1 unreachable=0 failed=1 node-1-master : ok=5 changed=1 unreachable=0 failed=1 node-2-slave-1 : ok=5 changed=1 unreachable=0 failed=1 2. Dry Run with --checkNext won’t execute the whole playbook, but will give a run through.1ansible-playbook foo.yml --check 3. Print Debugging Message with VarOf course you can print debug message with the fail module. However, it has a born behavior: stop the process, which maybe unexpected. Here we use the debug module to print statements during execution. Next is an example of printing the eth1 address of each nodes in the inventory file. 12345# more is here...- debug: msg: "hosts mapping is: &#123;&#123; hostvars[item]['ansible_eth1'].ipv4.address &#125;&#125; &#123;&#123;item&#125;&#125;" with_items: "&#123;&#123; groups['all'] &#125;&#125;" Here is the output of such debug msg:12345678910111213141516171819202122232425262728293031323334353637TASK [pre-ansible : debug] *****************************************************ok: [node-2-slave-1] =&gt; (item=node-1-master) =&gt; &#123; "item": "node-1-master", "msg": "hosts mapping is: 192.168.33.10 node-1-master"&#125;ok: [node-2-slave-1] =&gt; (item=node-2-slave-1) =&gt; &#123; "item": "node-2-slave-1", "msg": "hosts mapping is: 192.168.33.11 node-2-slave-1"&#125;ok: [node-2-slave-1] =&gt; (item=lcoj-judger) =&gt; &#123; "item": "lcoj-judger", "msg": "hosts mapping is: 192.168.33.12 lcoj-judger"&#125;ok: [node-1-master] =&gt; (item=node-1-master) =&gt; &#123; "item": "node-1-master", "msg": "hosts mapping is: 192.168.33.10 node-1-master"&#125;ok: [node-1-master] =&gt; (item=node-2-slave-1) =&gt; &#123; "item": "node-2-slave-1", "msg": "hosts mapping is: 192.168.33.11 node-2-slave-1"&#125;ok: [node-1-master] =&gt; (item=lcoj-judger) =&gt; &#123; "item": "lcoj-judger", "msg": "hosts mapping is: 192.168.33.12 lcoj-judger"&#125;ok: [lcoj-judger] =&gt; (item=node-1-master) =&gt; &#123; "item": "node-1-master", "msg": "hosts mapping is: 192.168.33.10 node-1-master"&#125;ok: [lcoj-judger] =&gt; (item=node-2-slave-1) =&gt; &#123; "item": "node-2-slave-1", "msg": "hosts mapping is: 192.168.33.11 node-2-slave-1"&#125;ok: [lcoj-judger] =&gt; (item=lcoj-judger) =&gt; &#123; "item": "lcoj-judger", "msg": "hosts mapping is: 192.168.33.12 lcoj-judger"&#125; In your play-book, please setup the gather_facts to be true. In this way, we can print the ipv4.adderss of this host.And the pre-ansible is the role where the above code snippet lies in. 1234567- hosts: all gather_facts: true sudo: yes roles: - pre-ansible tags: - pre-ansible Also, this debug module with msg can print an object with all its field values. So if our msg above changes to1"hosts mapping is: &#123;&#123; hostvars[item]['ansible_eth1'].ipv4 &#125;&#125; &#123;&#123;item&#125;&#125;" it will print more message. 4. Intro about Play-BookIn the section of Print Debugging Message with Var, we already saw one easy play, here is another playbook with only one play. Please remember we do have --- at the first line. 123456789101112131415161718192021---- hosts: webservers vars: http_port: 80 max_clients: 200 remote_user: root tasks: - name: ensure apache is at the latest version yum: name=httpd state=latest - name: write the apache config file template: src=/srv/httpd.j2 dest=/etc/httpd.conf notify: - restart apache - name: ensure apache is running (and enable it at boot) service: name=httpd state=started enabled=yes handlers: - name: restart apache service: name=httpd state=restarted 4.1. RolesWith Roles, we can reuse the tasks commands. For example, next play, we will execute the play with roles pre-ansible. Of course, we can execute more roles, just append the role directory name under roles field.12345- hosts: all gather_facts: true sudo: yes roles: - pre-ansible Here is a glance of directory structure, we can see pre-ansible directory in the roles directory.123456789101112131415161718192021222324[03:38 PM morganwu@promote noj-deploy]$ tree -L 2.├── ansible.cfg├── cluster.yml├── group_vars│ └── all.yml├── inventory.me├── roles│ ├── common│ ├── docker│ ├── etcd│ ├── flannel│ ├── kubernetes│ ├── kubernetes-addons│ ├── leetcode│ ├── leetcode-backend│ ├── master│ ├── nginx│ ├── node│ ├── opencontrail│ ├── opencontrail-provision│ └── pre-ansible├── setup.sh└── setup_leetcode.sh 4.1.1. Use Condition when Choosing RolesWe even can use condition expression when choosing specific roles,123456789- hosts: - etcd - masters - nodes sudo: yes roles: - &#123; role: flannel, when: networking == 'flannel' &#125; tags: - network-service-install This will only execute the roles of flannel when the networking varialbe is flannel. 4.2. TagsWith Tags, we can run specific play and it makes our dev-ops work more flexible, in a non-linear style. We will still using the example above. Here we have defined a tag pre-ansible1234567- hosts: all gather_facts: true sudo: yes roles: - pre-ansible tags: - pre-ansible When we execute playbook with --tags, it will only execute this play and skip all other plays without this specific tag.1[03:44 PM morganwu@promote noj-deploy]$ ansible-playbook -i inventory.me cluster.yml --tags=pre-ansible 4.2.1. Execute Multiple TagsIf you want to execute multiple tags once, just append with more tag name at the --tags, eg. 1[03:44 PM morganwu@promote noj-deploy]$ ansible-playbook -i inventory.me cluster.yml --tags="pre-ansible,etcd,docker" This will execute the pre-ansible, etcd, docker tags. 4.3. Var/String in a conditionHere is an example inventory_hostname is a varaible, but “codential” and “node-1-master” are all strings. The most important here is: the condition in the when, we use &#39; to wrap up. 12345678910111213141516- include: frontend.yml when: '(inventory_hostname in groups["codential"]) or (inventory_hostname == groups["nodes"][0])' vars: service_port: 8001 node_port: '&#123;&#123; cfg[env].node_port_frontend &#125;&#125;'- name: register nginx proxy become: true when: 'inventory_hostname == "node-1-master"' include: roles/leetcode/tasks/register-nginx-proxy.yml vars: service_port: 8001 node_port: '&#123;&#123; cfg[env].node_port_frontend &#125;&#125;' nginx_template: "frontend/&#123;&#123; env &#125;&#125;-nginx-conf.j2" nginx_conf: "/etc/nginx/conf.d/&#123;&#123; namespace &#125;&#125;-&#123;&#123; env &#125;&#125;.conf" iptables_comment: "&#123;&#123; namespace &#125;&#125;-&#123;&#123; env &#125;&#125;-node-port" 5. ConclusionAnsible is easy but powerful with lots of pre-defined modules. We can use fail module to stop execution process ann use debug module to print message with variables. Roles are designed for task reuuse. Tags are designed to execute the specific Play(s) in one playbook.]]></content>
      <categories>
        <category>Tool &amp; Skill</category>
        <category>DevOps</category>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
        <tag>Debug</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode Exercises]]></title>
    <url>%2F2017%2F06%2F13%2FLeetcode-Exercise%2F</url>
    <content type="text"><![CDATA[This is a leetcode algorithm list for me to track my thought and solutions. 1. Two-SumProblem Description: 1. Two-Sum 1.1. Golang123456789101112131415func twoSum(nums []int, target int) []int &#123; size := len(nums) for i := 0; i &lt; size; i++ &#123; for j := i + 1; j &lt; size; j++ &#123; if nums[i] + nums[j] == target &#123; result := []int&#123;i, j&#125; sort.Slice(result, func(i, j int) bool &#123; return result[i] &lt;= result[j] &#125;) return result &#125; &#125; &#125; return []int&#123;0, 0&#125;&#125; 2. Path SumProblem Description: 112. Path Sum 2.1. Java1234567891011121314151617181920212223/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */public class Solution &#123; /* Test if there is a path from current node to leaf node */ public boolean hasPathSum(TreeNode root, int sum) &#123; if(root == null) return false; if(root.left == null &amp;&amp; root.right == null) &#123; if (sum == root.val) return true; else return false; &#125; sum = sum - root.val; // take value from current root node return hasPathSum(root.left, sum) || hasPathSum(root.right, sum); &#125;&#125; 3. Reverse Words in a StringProblem Description: 151. Reverse Words in a String 3.1. Java1234567891011121314public class Solution &#123; public String reverseWords(String s) &#123; s = s.trim(); if(s.equals("")) return ""; String[] str_arr = s.split("\\s+"); StringBuilder sb = new StringBuilder(""); for(int i = str_arr.length-1; i&gt;0; i--) &#123; sb.append(str_arr[i] + " "); &#125; sb.append(str_arr[0]); return sb.toString(); &#125;&#125; 4. Combine Two TablesProblem Description: 175. Combine Two Tables 4.1. MySQL12345# Write your MySQL query statement belowselect p.FirstName, p.LastName, a.City, a.State from Person as p left join Address as a on p.PersonId = a.PersonId; 5. Fizz BuzzProblem Description: 412. Fizz Buzz 5.1. Java12345678910111213141516171819202122232425public class Solution &#123; private boolean multiple(int i, int modValue)&#123; return i%modValue==0; &#125; public List&lt;String&gt; fizzBuzz(int n) &#123; List&lt;String&gt; list = new ArrayList(); for(int i = 1;i &lt;= n;i++) &#123; String str = String.valueOf(i); if(multiple(i,3) &amp;&amp; multiple(i,5)) &#123; list.add("FizzBuzz"); &#125; else if(multiple(i,3) &amp;&amp; !multiple(i,5)) &#123; list.add("Fizz"); &#125; else if(!multiple(i,3) &amp;&amp; multiple(i,5)) &#123; list.add("Buzz"); &#125; else &#123; list.add(str); &#125; &#125; return list; &#125;&#125;]]></content>
      <categories>
        <category>Read &amp; Think</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simple Swift Intro]]></title>
    <url>%2F2017%2F06%2F12%2FSimple-Swift-Intro%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Emoji File Name under MacOSX]]></title>
    <url>%2F2017%2F06%2F09%2FEmoji-File-Name-under-MacOSX%2F</url>
    <content type="text"><![CDATA[Recently was too busy on small stuff that I can’t focus on myself on the real thing I need to tackle. Annoying… Anyway, I still find something fun which also stimulate my mind! Interesting! Ah! Here is my code of generating this:12345678910111213#!/bin/bash set -e for i in &#123;1..20&#125;do filename='👍 ' for (( j=1;j&lt;=$i;j++ )) do filename+='👍 ' done echo $filename touch "$filename"done More about Emoji…]]></content>
      <categories>
        <category>Fun</category>
        <category>Code</category>
        <category>Mac OSX</category>
      </categories>
      <tags>
        <tag>Emoji</tag>
        <tag>Mac OSX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transfer 15TB of Tiny Files]]></title>
    <url>%2F2017%2F06%2F05%2FTransfer-15TB-of-Tiny-Files%2F</url>
    <content type="text"><![CDATA[Days ago, when I was setting up an Internet Cafe System, I met a problem of transferring 600+ GB gaming files from one machine to another. Most of them are small files, I have two options: Compress and then transfer, which will cost lots of CPU, and of course time. Directly transfer these files, which will be very slow since too many tiny files. In the End, I used the rsync method. Relax time, here is the Windows start animation. 1. So, How?1.1. The Rsync methodMy scenario was transferring files from Windows XP to a Linux Server. About how to setup Cygwin in Windows XP/2003, please review Setup Cygwin in Windows XP/2003 12$ rsync -a -z -vvv -S -e 'ssh -o "ServerAliveInterval 7200"' \ /path/of/source_dir root@destination_server:/path/of/destination_dir This will transfer /path/of/source_dir to /path/of/destination_dir by using SSH tunnel and keep alive for 2 hours, using Rsync algorithm. 1.2. Compress &amp; Package &amp; TransferAs above, we mentioned that we would like to compress these files and then transfer. Here is how we implement it by using tar and pigz(parallel gzip) command which wins over the rsync method. Source Machine1$ tar -cf - -C /path/of/small/files . | pigz | nc -l 9876 Destination Machine (Download)1$ nc source_machine_ip 9876 | pigz -d | tar -xf - -C /put/stuff/here Destination Machine (Archive)1$ nc source_machine_ip 9876 &gt; smallstuff.tar.gz 2. But which one is better?The result is nc + pigz solution always win for the first time transfer, no matters for large files or small files. My laptop has 4 CPU cores built-in, so the rsync solution is about 3-4 times slower than the nc + pigz solution for the first-time transfer. However, for incremental files update, of course, rsync wins over another since it only compute the changes by using modification timestamp and size . Here is the script I used to generate 640,000 files with each as size of 8 kb. 1234567891011#!/bin/bashfor i in &#123;1..10&#125;; do for j in &#123;1..10&#125;; do mkdir -p "test_$i/test_$j" cd "test_$i/test_$j" for k in &#123;1..6400&#125;; do dd if=/dev/zero of=test_"$k" bs=1024 count=8 &gt; /dev/null 2&gt;&amp;1 &amp; done cd - donedone 3. Conclusion First Time Transfer, use pigz + nc Incremental Files Update, use rsync References: Transfer 15 TB of Tiny Files]]></content>
      <categories>
        <category>Tool &amp; Skill</category>
        <category>DevOps</category>
        <category>Backup</category>
      </categories>
      <tags>
        <tag>Rsync</tag>
        <tag>transfer</tag>
        <tag>copy</tag>
        <tag>nc</tag>
        <tag>many</tag>
        <tag>tiny</tag>
        <tag>file</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Setup Cygwin in Windows XP/2003]]></title>
    <url>%2F2017%2F06%2F04%2FSetup-Cygwin-in-Windows-XP-2003%2F</url>
    <content type="text"><![CDATA[Everyone should know how to setup Cygwin under Windows 7/10, however, if you do it in Windows XP/2003, it will failed.Why? Because the latest packages are only compatible with Windows 7 or higher.In other words, the Cygwin respository doesn’t have a mechanism to maintain different version of these binaries. So, how to setup Cygwin under Windows XP then? The good news is that someone keeps it. 1. Download the setup.exe32-bit: setup-x86-2.874.exe64-bit: setup-x86_64-2.874.exe 2. Run the setup.exe with -X optionPlease open the cmd to run with -X option setup-x86-2.874.exe -X 3. Use the legacy repositories32-bit: http://ctm.crouchingtigerhiddenfruitbat.org/pub/cygwin/circa/2016/08/30/10422364-bit: http://ctm.crouchingtigerhiddenfruitbat.org/pub/cygwin/circa/64bit/2016/08/30/104235 Here are the full list of Cygwin Time Machine Repository, please use the corresponding respository as the version of setup.exe32-bit: http://ctm.crouchingtigerhiddenfruitbat.org/pub/cygwin/circa/index.html64-bit: http://ctm.crouchingtigerhiddenfruitbat.org/pub/cygwin/circa/64bit/index.html Then always use this repository instead of others to install packages. Engjoy!!! References: Cygwin Time Machine, TL;DR]]></content>
      <categories>
        <category>Tool &amp; Skill</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Cygwin</tag>
        <tag>Windows XP</tag>
        <tag>Windows 2003</tag>
        <tag>Package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Useful Links]]></title>
    <url>%2F2017%2F06%2F03%2FUseful-Links%2F</url>
    <content type="text"><![CDATA[This is an assembly of useful links. Those Linux Command Example Explain Shell]]></content>
      <categories>
        <category>Tool &amp; Skill</category>
        <category>Knowledge Base</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Straight Through and Crossover Cable]]></title>
    <url>%2F2017%2F05%2F27%2FStraight-Through-and-Crossover-Cable%2F</url>
    <content type="text"><![CDATA[Recently I went Halifax to help a friend to setup Internet Cafe System, including the hardware and software. And I met the problem of make Internet Cables, i.e, Straight Through and Crossover and T568A/B color orders. There are two kinds of network cable that we already know, Straight Through and Crossover Cable. This article will give an summary of the differences and how to setup a connection in real by giving photos. 1. Straight Through and Crossover1.1. T568A and T568B StandardsPlease note the only differences are the sequences order of different lines with different colors. 1.2. Straight Through and CrossoverThe Straight Through cable will use the same sequence order in both side of a cable, either use the same T568A or T568B order.Normally, T568B order has a better performance. Next picture is the Straight Through with T568B order. The Crossover cable will use the opposite sequence order in different sides, i.e., one side use T568A and another use T568A, or vice versa.Next shows a crossover example. 1.3. When to use which cable?Straight Through cable is primarily used for connecting unlike devices, and Crossover cable is used for connecting alike devices.Use straight through cable for the following cabling: Switch to router Switch to PC or server Hub to PC or server Home Router to PC (a Home Router normally contains a switch and a router) Use crossover cables for the following cabling: Switch to switch Switch to hub Hub to hub (Switch and Hub are alike devices) Router to router Router Ethernet port to PC NIC PC to PC 2. How to Setup?2.1. Things You Need 2.2. Take off the Skin of the Cable 2.3. Places the Lines as T568A/T568B Order 2.4. Cut Cable to Make Lines Neat 2.5. Push Lines into RJ45 Port 2.6. Use Piller to Hold Down the Metel Sheets on RJ45 References: Difference Between Straight Through and Crossover Cable How to make Network Cable]]></content>
      <categories>
        <category>Tool &amp; Skill</category>
        <category>Gaming Club Ops</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Cable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH Tricks]]></title>
    <url>%2F2017%2F05%2F18%2FSSH-Tricks%2F</url>
    <content type="text"><![CDATA[You sometimes would see somebody doing something cool with SSH(Secure Shell), like without password, or even without key, or even more, port forwarding to break the firewall restrictions. This post will discuss ssh tricks of authentications methods and port forwarding approaches. Three kinds of SSH authentication methods: Password Public/private key pair Host-based authentication There are three kinds of SSH Forwarding: Local Port Forwarding Remote Reverse Forwarding Dynamic Port Forwarding This post will make these stuff more clear. 1. SSH Authentication MethodsWe won’t talk about the password methods since it’s too easy. 1.1. Public/Private Keypair, free password Step 1: Genrate Keys by typing ssh-keygen from you server1. It will generate ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub key pairs which are private key/public keys of your server1 of the user whom you’re currently logged as. private key should be kept secret public key is meant to be shared Step 2: Copy the content of public key to remote server’s ~/.ssh/authorized_keys, by appending this file. Step 3: Login from server1 to remote server, you don’t need password now! 1.2. Host-based Authentication Doesn’t need user credentials(password or key), actually we just write it in a configuration file, it’s kind of alias Provides trust based on hostname and userid Userid on both system has to be the same Here is an example that login into server1 vagrant box:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Here we list the hostname and its config[09:51 PM morganwu@morgan-yinnut ~]$ cat ~/.ssh/config |grep -A6 srv1 Host srv1 User vagrant Port 2222 Hostname 127.0.0.1 IdentityFile /Users/morganwu/Developer/workspace/ssh_port_forward/server1/.vagrant/machines/default/virtualbox/private_key StrictHostKeyChecking no# Here we just ssh with an alias without username and hostname[09:51 PM morganwu@morgan-yinnut ~]$ ssh srv1@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the ECDSA key sent by the remote host isSHA256:jYEhZ8yXtOJFowmQnMyA+bHshrhX7H30vVQF9UyND48.Please contact your system administrator.Add correct host key in /Users/morganwu/.ssh/known_hosts to get rid of this message.Offending ECDSA key in /Users/morganwu/.ssh/known_hosts:247Password authentication is disabled to avoid man-in-the-middle attacks.Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 3.13.0-112-generic x86_64) * Documentation: https://help.ubuntu.com/ System information as of Fri May 19 01:50:52 UTC 2017 System load: 0.28 Processes: 78 Usage of /: 3.6% of 39.34GB Users logged in: 0 Memory usage: 25% IP address for eth0: 10.0.2.15 Swap usage: 0% Graph this data and manage this system at: https://landscape.canonical.com/ Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.New release '16.04.2 LTS' available.Run 'do-release-upgrade' to upgrade to it.Last login: Fri May 19 01:50:53 2017 from 10.0.2.2vagrant@vagrant-ubuntu-trusty-64:~$ 2. SSH Forwarding2.1. SSH Local Normal Port ForwardingHere is the scenario why we need Port Forwarding: bypass the private network, or using the Jumpbox. Now we want to access web server on box2 via box1. Here is how we create a local port forward. Now we can type localhost:8000 from your-box, this will forward the request to box-2:80 via box-1 ssh tunnel created above!If we have multiple box-2 here, this is the load balancing model. We call this as local port forwarding instead of remote port forwarding, since the setup direction is the same as the resource access direction. This is also called the normal tunnel instead of the reverse tunnel via ssh. This can be done by following three steps: setup ssh connection from your-box to box-1 box-1 forward the requets to box-2 so you can access box-2 from your-box now even box-2 is in the internal network Run the command ssh -L localport:DEST_HOST:DEST_PORT VIA_HOST locally and then access local localport to get the DEST_HOST:DEST_PORT content.This will open a localport listened locally, to serve all requests from local and then forward them to DEST_HOST:DEST_PORT by using the ssh tunnel from local to the VIA_HOST. 2.1.1. Share Your SSH Local ForwardingWe could enable localport to be accessed by other machines by appending the -g switch. Next we enable local port forwarding from localhost:8000 to ece.uwaterloo.ca:80 via ecelinux4.uwaterloo.ca:22 12345678910111213141516171819[11:23 PM morganwu@morgan-yinnut proxies]$ ssh -L 8000:ece.uwaterloo.ca:80 -g m92wu@ecelinux4.uwaterloo.caLast login: Thu May 18 23:23:43 2017 from cpebc4dfb93ed53-cmbc4dfb93ed50.cpe.net.cable.rogers.comECE Department Linux NoMachine ServerThis server is ONLY to be used to access other Linux servers.Course software should NOT be run on this server.PLEASE USE ssh -X eceLinuxN TO LOG INTO OTHER LINUX SERVERS SUCH AS eceLinux1, eceLinux2, eceLinux3, eceLinux5 .. eceLinux11 and run course software there.*** THANK YOU TO WEEF for purchasing this server! ***This machine is a NoMachine server with a 4-core 3.8GHz AMD FX CPU with 32G of ECC RAMThe login password is synchronized with Nexus.Forward any questions to Eric in E2-2357 or sysadmins@ecemail.uwaterloo.ca[m92wu@ecelinux4 ~]$ Here lists the port information, we can see 8080 is bound to all inet interfaces. 123[11:23 PM morganwu@morgan-yinnut proxies]$ netstat -na|grep tcp|grep 8000tcp4 0 0 *.8000 *.* LISTEN tcp6 0 0 *.8000 *.* LISTEN Now let’s access the ece.uwaterloo.ca:80 page from another private virtual machine on my local computer. We can see it could have access to that page now. 12345678vagrant@vagrant-ubuntu-trusty-64:~$ curl 192.168.0.12:8000&lt;!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;302 Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Found&lt;/h1&gt;&lt;p&gt;The document has moved &lt;a href="https://ece.uwaterloo.ca/"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; Here is an image for demonstrating this: 2.1.2. Use Compression via this tunnelBy using -C together with -L option. 2.2. SSH Remote Reverse Port ForwardingLocal Port Forwarding has the same direction of SSH tunnel as the request forwarding, while Remote Reverse Forwarding does the opposite direction. So you setup a ssh tunnel from your-box to box-1 which has a public address while your-box doesn’t. But you would like to access your-box from box-2 which is outside. Now you can only do this via box-1. If we have multiple your-box here, this is the so-called reverse proxy load balancing model. So our command format is ssh -R VIA_LISTEN_PORT:DEST_HOST:DEST_PORT VIA_HOST,This command will: open a ssh tunnel from DEST_HOST to VIA_HOST listen a VIA_LISTEN_PORT on the VIA_HOST forward all requests from VIA_HOST:VIA_LISTEN_PORT to DEST_HOST:DEST_PORT Also, the -C option will work here. 2.3. SSH Dynamic Port ForwardingIn the SSH-Local-Normal-Port-Forwarding, we already know we can forward localhost:8000 request to remote_server:port via a remote public server, but this is only for specific port. What if we want to forward every kind of requests? This is the idea of dynamic port forwarding, which is also be called as a proxy. 1$ ssh -D 9999 -C m92wu@ecelinux4.uwaterloo.ca The above command will forward every requests from localhost if you setup this socks proxy as the proxy of your client. References: https://www.slideshare.net/zdennis/ssh-9115832]]></content>
      <categories>
        <category>Tool &amp; Skill</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>port forwarding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Read List]]></title>
    <url>%2F2017%2F05%2F16%2FRead-List%2F</url>
    <content type="text"><![CDATA[Keep Updating to add new articles and papers to read categories methodology style https://stephenholiday.com/notes/ https://cs.stanford.edu/~matei/courses/2015/6.S897 Distributed SystemIf can’t find something new to read, consider these sites: https://cs.stanford.edu/~matei/courses/2015/6.S897/ https://pdos.csail.mit.edu/6.824/schedule.html https://dancres.github.io/Pages/ 1. Language1.1. JavaJava 8 Labmda, Streams https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html http://www.oracle.com/technetwork/articles/java/architect-lambdas-part1-2080972.html http://www.oracle.com/technetwork/articles/java/architect-lambdas-part2-2081439.html https://www.ibm.com/developerworks/java/library/j-java-streams-1-brian-goetz/index.html http://www.oracle.com/technetwork/articles/java/ma14-java-se-8-streams-2177646.html http://www.oracle.com/technetwork/articles/java/architect-streams-pt2-2227132.html 2. NetworkBBR http://queue.acm.org/detail.cfm?id=3022184 https://www.ietf.org/proceedings/97/slides/slides-97-iccrg-bbr-congestion-control-02.pdf https://www.zhihu.com/question/53559433 https://www.zhihu.com/question/53858229 3. RecoveryBookKeeper and HedWig http://dl.acm.org/citation.cfm?id=2433144 https://cwiki.apache.org/confluence/display/BOOKKEEPER/BookKeeper http://bookkeeper.apache.org/ http://blog.csdn.net/liuhong1123/article/details/8945456 http://dockone.io/article/78 4. StreamingKafka: a Distributed Messaging System for Log Processing - LinkedIn 2011, notes http://people.csail.mit.edu/matei/courses/2015/6.S897/readings/kafka.pdf 5. Container &amp; DevOpsBorg, Omega, and Kubernets http://www.e-wilkes.com/john/papers/2016-Queue-Kubernetes.pdf 6. Consistency and Locks基于Redis的分布式锁到底安全么？ http://zhangtielei.com/posts/blog-redlock-reasoning.html Consistency: From Chubby to Paxos and Raft 7. CacheGuava Cache 8. OLTP, OLAP, DatabaseHybrid Transactional/Analytical Processing: A Survey http://delivery.acm.org/10.1145/3060000/3054784/p1771-ozcan.pdf Kudu: Storage for Fast Analytics on Fast Data https://kudu.apache.org/kudu.pdf 9. Time Series DatabaseTSDB, OpenTSDB http://btw2017.informatik.uni-stuttgart.de/slidesandpapers/E4-14-109/paper_web.pdf ftp://ftp.informatik.uni-stuttgart.de/pub/library/medoc.ustuttgart_fi/DIP-3729/DIP-3729.pdf]]></content>
      <categories>
        <category>Read &amp; Think</category>
        <category>Knowledge Base</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka: a Distributed Messaging System for Log Processing - LinkedIn 2011]]></title>
    <url>%2F2017%2F05%2F14%2FKafka-a-Distributed-Messaging-System-for-Log-Processing-LinkedIn-2011%2F</url>
    <content type="text"><![CDATA[Not complete yet. Kafka is a Distributed Streaming platform. Kafka™ is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies. The Kafka site said it has three main functionalities: PUBLISH &amp; SUBSCRIBE to streams of data like a messaging system PROCESS streams of data efficiently and in real time STORE streams of data safely in a distributed replicated cluster References: http://people.csail.mit.edu/matei/courses/2015/6.S897/readings/kafka.pdf]]></content>
      <categories>
        <category>Read &amp; Think</category>
        <category>Message Passing</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Message Queue</tag>
        <tag>Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F04%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 1. Quick Start1.1. Create a new post1$ hexo new "My New Post" More info: Writing 1.2. Run server1$ hexo server More info: Server 1.3. Generate static files1$ hexo generate More info: Generating 1.4. Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>